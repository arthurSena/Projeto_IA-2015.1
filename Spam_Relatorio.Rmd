---
title: "Spam Relatorio"
author: "Arthur Sena, Bruna Amorim e Leonardo Alves"
date: "11/07/2015"
output: html_document
---

```{r  message=FALSE, warning=FALSE}
library(plyr, quietly = T, warn.conflicts = F)
library(ggplot2, quietly = T, warn.conflicts = F)
library(dplyr, quietly = T, warn.conflicts = F)
library("GGally", quietly = T, warn.conflicts = F)
library("gmodels", quietly = T, warn.conflicts = F)
library("kernlab", quietly = T, warn.conflicts = F)
library(caret, quietly = T, warn.conflicts = F)
require("grid")
require("C50")
```

#Introdução
__Universidade Federal de Campina Grande - DSC  __  
__Disciplina: Inteligência Artificial  __  
__Professor: Herman Martins Gomes  __  
__Grupo:__ Arthur Sena/Bruna Amorim/Leonardo Alves  

##Objetivo
O nosso projeto tem como objetivo comparar três modelos de "Machine learning" a respeito da performance deles quanto a predição de e-mails como spam ou não spam. Usaremos as métricas Recall, Precision e F-measure a fim de comparar seus desempenhos. Os modelos escolhidos para nosso experimento estão descritos logo abaixo.

__Árvores de Decisão__  
__KNN Model__  
__SVM__  

##Principais Ferramentas
[R version 3.2.1](http://www.r-project.org/) - Ferramenta para análise estatística.   
[Openintro Package](http://cran.r-project.org/web/packages/openintro/openintro.pdf#Rfn.email50) - Pacote, onde foram obtidos os emails.  
[Caret Package](http://caret.r-forge.r-project.org/) - Pacote usado para treinar e testar os nossos modelos.  

Obs.: Será utilizado alguns pacotes adicionais para facilitar a manipulação e visualiazação dos dados.

##Dados
Vamos utilizar um conjunto de dados que representam informações sobre uma coleção de e-mails classificados como spam ou não spam. Tais dados foram obtidos a partir do pacote "openintro". Cada email do conjunto apresenta o seguinte conjunto de variáveis:

![alt text](Data/emails_descricoes.png)

Uma pequena amostra dos nossos dados jutamante com algumas variáveis pode ser visualizada logo abaixo:

```{r  message=FALSE, warning=FALSE}
  emails <- read.csv("Data/emails.csv")
  head(emails[,c(1,3,4,5,7,8,13,14,15)])
```

##Análise dos dados
Antes de fazer qualquer tipo de construção de modelo devemos analisar nossos dados e verificar quais variáveis apresentam uma correlação mais forte com a variável spam. Utilizaremos a correlação linear a fim de fazer tal checagem. Lembrando que quanto mais perto de zero mais fraca é a correlação entre as variáveis. Caso o valor seja próximo de -1, então temos uma correlação negativa e quanto mais próximo de 1 uma correlação positiva. 

```{r  message=FALSE, warning=FALSE}
set.seed(10)
#Fazendo alguns ajustes nos nossos dados.
emails[,"time"] <- as.character(emails[,"time"]) 
emails[,"time"] <- as.numeric(as.POSIXct(emails[,"time"]))
levels(emails[,"number"]) <- c(1,0,-1)
emails[,"number"] <- as.numeric(emails[,"number"])
emails[,"winner"] <- as.numeric(emails[,"winner"]) 





df <- as.data.frame(cor(emails)[,1])
df <- cbind(Variaveis = rownames(df), df)
rownames(df) <- NULL
colnames(df) <- c("Variaveis","Correlacao")
df <- df[-1,]
ggplot(df, aes(x = Variaveis, y = Correlacao),width = 5) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle=45, vjust=1))
```

O gráfico de barras acima mostra a correlação de nossas variáveis com a variável spam. Notamos que algumas variáveis se sobressaem sobre outras. Tais variáveis são:

* Format  
* To_Multiple   
* Line_breaks   
* Number  
* Num_char  
* Re_subj  
* Sent_email   

Desse modo, usaremos como base tais variáveis para construir nossos modelos de predição.

##Criando e treinando nossos modelos - Primeiros Modelos
Nessa parte do relatório, vamos usar uma biblioteca do R conhecida como "Caret". Tal biblioteca apresenta um grande quantidade de algoritmos de classificação implementados. Assim sendo, vamos usar a técnica de crossvalidation para criar nossos modelos, pois queremos evitar o 'overfitting'. Antes de iniciar o treinamento dos modelos vamos dividir os emails em um conjunto de treinamento e de testes.

```{r warning=FALSE, message=FALSE}
spam <- filter(emails, spam == 1)
nao_spam <- filter(emails, spam == 0) 

#Criando conjunto de treino. 75% 
treino <- rbind(spam,nao_spam[1:2573,])
treino[,"sent_email"] <- as.factor(treino[,"sent_email"])
treino[,"to_multiple"] <- as.factor(treino[,"to_multiple"])
treino[,"format"] <- as.factor(treino[,"format"])
treino[,"re_subj"] <- as.factor(treino[,"re_subj"])
#Criando conjunto de testes. 25%
teste<- rbind(spam,nao_spam[2574:3554,])
teste[,"sent_email"] <- as.factor(teste[,"sent_email"])
teste[,"to_multiple"] <- as.factor(teste[,"to_multiple"])
teste[,"format"] <- as.factor(teste[,"format"])
teste[,"re_subj"] <- as.factor(teste[,"re_subj"])
```

Dividimos nosso conjunto de forma que 75% dos dados se tornaram nosso treino e os 25% restantes nossos testes. Agora podemos iniciar o treinamento dos nossos modelos.

```{r  message=FALSE, warning=FALSE}
treino[,"spam"]<- as.factor(treino$spam)
#Usando o KNN
control = trainControl(method = "repeatedcv", number=10,repeats = 10)
knn_grid = expand.grid(k=c(1:20))
knn_model = train(spam~num_char+sent_email+to_multiple+format+line_breaks+number+re_subj,  data=treino ,method="knn",preProcess=c("range"),tuneGrid = knn_grid,trControl=control, na.action=na.omit)

#Usando o SVM
### Encontrando o melhor valor para um parametro de tuning
sigDist <- sigest(spam~num_char+sent_email+to_multiple+format+line_breaks+number+re_subj, data = treino, frac = 1)
svmTuneGrid <- data.frame(.sigma = sigDist[1], .C = 2^(-2:7))
control <- trainControl(method="repeatedcv", number=10,repeats=10)
modelSvm <- train(spam~num_char+sent_email+to_multiple+format+line_breaks+number+re_subj, data=treino, method="svmRadial", preProc = c("center", "scale"),tuneGrid = svmTuneGrid, trControl=control)

#Usando C50
control <- trainControl(method="repeatedcv", number=10, repeats=10)
modelC50<- train(spam~num_char+sent_email+to_multiple+format+line_breaks+number+re_subj, data=treino, method="C5.0", trControl=control)
```

#Analisando a importancia das variáveis
Podemos utilizar a função "vaiImp" a fim de verificar a importancia de cada variável em cada modelo.
```{r}
knn_import <- varImp(knn_model, scale = T)
plot(knn_import)

svm_import <- varImp(modelSvm, scale = T)
plot(svm_import)

c50_import <- varImp(modelC50, scale = T)
plot(c50_import)
```

#Primeira Predição
Com os modelos criados podemos fazer nossa primeira predição em cima dos dados de teste e, logo após, verificar o desempenho dos classifcadores.

```{r  message=FALSE, warning=FALSE}
teste[,"spam"]<- as.factor(teste$spam)
resultados_knn <- data.frame()
resultados_c50 <- data.frame()
resultados_svm <- data.frame()
#KNN
pred_knn<-predict(knn_model, newdata = as.data.frame(teste),na.action=na.omit)
CrossTable(teste$spam, pred_knn, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 362/428
 recall <- 362/367
 Fmeasure_knn <-  2 * precision * recall / (precision + recall)
 resultados_knn<-rbind(resultados_knn,data.frame(Predicao = 1, Prec = precision, Recall = recall,Fmeasure = Fmeasure_knn)) 

#SVM
pred_svm<-predict(modelSvm, newdata = teste)
CrossTable(teste$spam, pred_svm, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 63/73
 recall <- 63/367
 Fmeasure_svm <-  2 * precision * recall / (precision + recall)
 resultados_svm<-rbind(resultados_svm,data.frame(Predicao = 1, Prec = precision, Recall = recall,Fmeasure = Fmeasure_svm)) 

#C50
pred_c50<-predict(modelC50, newdata = teste)
CrossTable(teste$spam, pred_c50, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 225/272
 recall <- 225/367
 Fmeasure_c50 <-  2 * precision * recall / (precision + recall)
 resultados_c50<-rbind(resultados_c50,data.frame(Predicao = 1, Prec = precision, Recall = recall,Fmeasure = Fmeasure_c50)) 

Modelos = c("ÁrvoreDec", "KNN", "SVM") 
Fmeasures = c(Fmeasure_c50,Fmeasure_knn,Fmeasure_svm)
df = data.frame(Modelos, Fmeasures)  
ggplot(df, aes(x = Modelos, y = Fmeasures)) + geom_bar(stat="identity")
```

#Quantidade Igual de emails spam e not-spam - Segunda Predição
Nesse cenário, modificaremos nosso conjunto de testes de modo que ele apresente um quantidade igual de emails spam e not-spam.
```{r}
temp1 <- filter(teste, spam == 0)[1:400,]
temp2 <- filter(teste, spam == 1)
teste_proporcional <- rbind(temp1,temp2)
```
Feito isso, vamos prever novamente nosso teste e observar como nossos modelos iniciais se comportam.

```{r}
teste_proporcional[,"spam"]<- as.factor(teste_proporcional$spam)
#KNN
pred_knn<-predict(knn_model, newdata = as.data.frame(teste_proporcional),na.action=na.omit)
CrossTable(teste_proporcional$spam, pred_knn, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 363/390
 recall <- 363/367
 Fmeasure_knn <-  2 * precision * recall / (precision + recall)
 resultados_knn<-rbind(resultados_knn,data.frame(Predicao = 2, Prec = precision, Recall = recall,Fmeasure = Fmeasure_knn)) 

#SVM
pred_svm<-predict(modelSvm, newdata = teste_proporcional)
CrossTable(teste_proporcional$spam, pred_svm, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 63/66
 recall <- 63/367
 Fmeasure_svm <-  2 * precision * recall / (precision + recall)
 resultados_svm<-rbind(resultados_svm,data.frame(Predicao = 2, Prec = precision, Recall = recall,Fmeasure = Fmeasure_svm)) 

#C50
pred_c50<-predict(modelC50, newdata = teste_proporcional)
CrossTable(teste_proporcional$spam, pred_c50, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 225/236
 recall <- 225/367
 Fmeasure_c50 <-  2 * precision * recall / (precision + recall)
 resultados_c50<-rbind(resultados_c50,data.frame(Predicao = 2, Prec = precision, Recall = recall,Fmeasure = Fmeasure_c50)) 

Modelos = c("ÁrvoreDec", "KNN", "SVM") 
Fmeasures = c(Fmeasure_c50,Fmeasure_knn,Fmeasure_svm)
df = data.frame(Modelos, Fmeasures)  
ggplot(df, aes(x = Modelos, y = Fmeasures)) + geom_bar(stat="identity")
```

Observando o gráfico, notamos que alguns de nossos modelos apresentaram uma certa queda de desempenho.

#Utilizando Todas as Variáveis - Segundo Modelo
Nesse cenário, nós utilizaremos todos as variáveis do modelo e observaremos o resultado
```{r  message=FALSE, warning=FALSE}
treino[,"spam"]<- as.factor(treino$spam)
#Usando o KNN
control = trainControl(method = "repeatedcv", number=10,repeats = 10)
knn_grid = expand.grid(k=c(1:20))
knn_model = train(spam~.,  data=treino ,method="knn",preProcess=c("range"),tuneGrid = knn_grid,trControl=control, na.action=na.omit)

#Usando o SVM
### finding optimal value of a tuning parameter
sigDist <- sigest(spam~num_char+sent_email+to_multiple+format+line_breaks+number+re_subj, data = treino, frac = 1)
### creating a grid of two tuning parameters, .sigma comes from the earlier line. we are trying to find best value of .C
svmTuneGrid <- data.frame(.sigma = sigDist[1], .C = 2^(-2:7))
control <- trainControl(method="repeatedcv", number=10,repeats=10)
modelSvm <- train(spam~., data=treino, method="svmRadial", preProc = c("center", "scale"),tuneGrid = svmTuneGrid, trControl=control)

#Usando C50
control <- trainControl(method="repeatedcv", number=10, repeats=10)
modelC50<- train(spam~., data=treino, method="C5.0", trControl=control)
```

#Analisando a importancia das variáveis
Podemos utilizar a função "vaiImp" a fim de verificar a importancia de cada variável em cada modelo.
```{r}
knn_import <- varImp(knn_model, scale = T)
plot(knn_import)

svm_import <- varImp(modelSvm, scale = T)
plot(svm_import)

c50_import <- varImp(modelC50, scale = T)
plot(c50_import)
```


#Utilizando Todas as Variáveis - Terceira Predição
Agora vamos realizar nossas predições com os novos modelos criados.
```{r  message=FALSE, warning=FALSE}
teste[,"spam"]<- as.factor(teste$spam)
#KNN
pred_knn<-predict(knn_model, newdata = as.data.frame(teste),na.action=na.omit)
CrossTable(teste$spam, pred_knn, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <-276/668
 recall <- 276/367
 Fmeasure_knn <-  2 * precision * recall / (precision + recall)
 resultados_knn<-rbind(resultados_knn,data.frame(Predicao = 3, Prec = precision, Recall = recall,Fmeasure = Fmeasure_knn)) 

#SVM
pred_svm<-predict(modelSvm, newdata = teste)
CrossTable(teste$spam, pred_svm, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 262/753
 recall <- 262/367
 Fmeasure_svm <-  2 * precision * recall / (precision + recall)
 resultados_svm<-rbind(resultados_svm,data.frame(Predicao = 3, Prec = precision, Recall = recall,Fmeasure = Fmeasure_svm)) 

#C50
pred_c50<-predict(modelC50, newdata = teste)
CrossTable(teste$spam, pred_c50, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 312/1288
 recall <- 312/367
 Fmeasure_c50 <-  2 * precision * recall / (precision + recall)
 resultados_c50<-rbind(resultados_c50,data.frame(Predicao = 3, Prec = precision, Recall = recall,Fmeasure = Fmeasure_c50)) 

Modelos = c("ÁrvoreDec", "KNN", "SVM") 
Fmeasures = c(Fmeasure_c50,Fmeasure_knn,Fmeasure_svm)
df = data.frame(Modelos, Fmeasures)  
ggplot(df, aes(x = Modelos, y = Fmeasures)) + geom_bar(stat="identity")
```

Notamos que nossos modelos, principalmente o SVM, apresentaram uma quantidade de acertos maior, porém eles também começaram a errar mais do que anteriormente o que no nosso contexto é um erro bem grave.

#Utilizando Todas as Variáveis - Quarta Predição
Vamos novamente utilizar nossos modelos para prever um conjunto de dados mais balanceado de e-mails.
```{r warning=FALSE, message=FALSE}
teste_proporcional[,"spam"]<- as.factor(teste_proporcional$spam)
#KNN
pred_knn<-predict(knn_model, newdata = as.data.frame(teste_proporcional),na.action=na.omit)
CrossTable(teste_proporcional$spam, pred_knn, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 276/374
 recall <- 276/367
 Fmeasure_knn <-  2 * precision * recall / (precision + recall)
 resultados_knn<-rbind(resultados_knn,data.frame(Predicao = 4, Prec = precision, Recall = recall,Fmeasure = Fmeasure_knn)) 

#SVM
pred_svm<-predict(modelSvm, newdata = teste_proporcional)
CrossTable(teste_proporcional$spam, pred_svm, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 262/373
 recall <- 262/367
 Fmeasure_svm <-  2 * precision * recall / (precision + recall)
 resultados_svm<-rbind(resultados_svm,data.frame(Predicao = 4, Prec = precision, Recall = recall,Fmeasure = Fmeasure_svm)) 

#C50
pred_c50<-predict(modelC50, newdata = teste_proporcional)
CrossTable(teste_proporcional$spam, pred_c50, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual', 'predicted'))
 precision  <- 316/367
 recall <- 316/367
 Fmeasure_c50 <-  2 * precision * recall / (precision + recall)
 resultados_c50<-rbind(resultados_c50,data.frame(Predicao = 4, Prec = precision, Recall = recall,Fmeasure = Fmeasure_c50)) 

Modelos = c("ÁrvoreDec", "KNN", "SVM") 
Fmeasures = c(Fmeasure_c50,Fmeasure_knn,Fmeasure_svm)
df = data.frame(Modelos, Fmeasures)  
ggplot(df, aes(x = Modelos, y = Fmeasures)) + geom_bar(stat="identity")
```

#Resultados e Conclusões
Para resumir todos os resultados, nós construímos a tabela abaixo que resume o que obtivemos de valores para cada métrica com os nossos modelos executando em diferentes cenários.

```{r}
resultados_c50$Model <- c("c50","c50","c50","c50")
resultados_knn$Model <- c("knn","knn","knn","knn")
resultados_svm$Model <- c("svm","svm","svm","svm")

resultado_final <- rbind(resultados_c50,resultados_knn,resultados_svm)

ggplot(data=resultado_final, aes(x=Predicao, y=Prec,group=Model,colour=Model)) +
    geom_line() + geom_point()

ggplot(data=resultado_final, aes(x=Predicao, y=Recall,group=Model,colour=Model)) +
    geom_line() + geom_point()

ggplot(data=resultado_final, aes(x=Predicao, y= Fmeasure,group=Model,colour=Model)) +
    geom_line() + geom_point()
```

Visualizando os resultados, acreditamos que a conclusão mais importante é que as métricas de nossos modelos tendem a melhorar ou piorar dependendo do cenário em que os modelos foram testados ou configurados. Os gráficos apresentados no relatório técnico dão suporte a nossa conclusão.